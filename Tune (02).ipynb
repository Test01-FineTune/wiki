{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6529f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Install Libraries\n",
    "!pip install accelerate==0.34.2 transformers==4.44.2 datasets==2.20.0 torch==2.3.1 -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186546e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2Ô∏è‚É£ Imports and Configuration\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "BLOCK_SIZE = 128 # The fixed length we want for every training example\n",
    "\n",
    "print(\"‚è≥ Loading Model and Tokenizer...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set the pad_token for batching (standard for GPT-like models)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(\"‚úÖ Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5851c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Load Simple Text Dataset (Wikitext-2)\n",
    "print(\"‚è≥ Loading Wikitext-2 dataset...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Remove unnecessary columns and tokenize the raw text\n",
    "def tokenize_function(examples):\n",
    "    # Process the 'text' field\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=4, # Use multiple processes for faster tokenization\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Group Texts into Fixed Blocks\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts into one long list of tokens\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # Drop the last partial block\n",
    "    total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n",
    "    \n",
    "    # Split the long list into chunks of BLOCK_SIZE\n",
    "    result = {\n",
    "        k: [t[i : i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create labels: for Causal LM, the input sequence is shifted to become the label\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Apply the grouping function\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "# 5Ô∏è‚É£ Data Collator (Ensures MLM is False for Causal LM)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "print(f\"üìù Final training sample size: {len(lm_datasets['train'])}\")\n",
    "print(\"‚úÖ Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241754e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Use a very small subset for a fast demonstration run (around 100 steps)\n",
    "train_subset = lm_datasets[\"train\"].select(range(2000))\n",
    "eval_subset = lm_datasets[\"validation\"].select(range(500))\n",
    "\n",
    "# 6Ô∏è‚É£ Define Training Arguments (Minimal settings)\n",
    "OUTPUT_DIR = \"./simple_clm_results\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,          # Only 1 epoch\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=50,            # Log loss every 50 steps\n",
    "    save_strategy=\"no\",          # Don't save checkpoints during this quick run\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 7Ô∏è‚É£ Initialize and Train the Trainer\n",
    "print(\"üöÄ Initializing Trainer and starting fine-tuning...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=eval_subset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"‚úÖ Fine-tuning complete!\")\n",
    "\n",
    "# 8Ô∏è‚É£ Test Inference\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "prompt = \"The primary goal of machine learning is to\"\n",
    "print(f\"\\nInput Prompt: {prompt}\\n\")\n",
    "\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True, \n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(\"üìù Generated Text:\")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e877871",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0a2e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead663f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503b27e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc101f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7be54f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639f83a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c849e6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9b9f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186eeec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c37b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8405f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1e44e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ee5a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce298e88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd6b78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c9efc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb604a4d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229f417",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183cc14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
