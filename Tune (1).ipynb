{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d005cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Install Libraries (Same as before, good to keep)\n",
    "!pip install accelerate==0.34.2 transformers==4.44.2 datasets==2.20.0 torch==2.3.1 -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2fccb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Uninstall the potentially conflicting packages\n",
    "# '--yes' forces the uninstallation without prompting\n",
    "!pip uninstall transformers accelerate datasets torch -y\n",
    "\n",
    "# 2. Reinstall the necessary, compatible versions\n",
    "# Use the exact, known-good versions:\n",
    "!pip install accelerate==0.34.2 transformers==4.44.2 datasets==2.20.0 torch==2.3.1 -U\n",
    "\n",
    "# 3. Import Check\n",
    "# After the installation, you MUST restart your runtime (e.g., in Colab: Runtime -> Restart Session)\n",
    "# Then, try re-running the modified code, starting with the imports.\n",
    "\n",
    "print(\"‚úÖ Installation and reinstallation commands executed.\")\n",
    "print(\"üö® Please RESTART YOUR RUNTIME (e.g., Runtime > Restart session in Colab) and then run all cells again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2Ô∏è‚É£ Imports and Model/Tokenizer Loading (Modified)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "# from transformers import login # Keep this if you plan to upload later\n",
    "\n",
    "# We use the full 'gpt2' model for better results\n",
    "MODEL_NAME = \"gpt2\"\n",
    "\n",
    "# 3Ô∏è‚É£ Load Model and Tokenizer\n",
    "print(f\"‚è≥ Loading Model: {MODEL_NAME}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set the padding token. This is important for GPT-like models when batching sequences.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Use the End Of Sentence token as the pad token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(\"‚úÖ Model and Tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3405862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Ô∏è‚É£ Load New Dataset\n",
    "from datasets import load_dataset\n",
    "# Using 'daily_dialog' dataset for conversational training\n",
    "print(\"‚è≥ Loading DailyDialog dataset...\")\n",
    "dataset = load_dataset(\"daily_dialog\")\n",
    "print(dataset) # Inspect the structure\n",
    "\n",
    "# 5Ô∏è‚É£ Data Preprocessing and Tokenization (Modified)\n",
    "MAX_LENGTH = 512 # Max length of the input sequence\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # The 'dialog' field is a list of strings (turns in a conversation).\n",
    "    # We join them into one continuous text string for the Causal LM task.\n",
    "    full_text = [\" \".join(dialog_list) for dialog_list in examples[\"dialog\"]]\n",
    "    \n",
    "    # Tokenize the combined text\n",
    "    return tokenizer(full_text, \n",
    "                     truncation=True, \n",
    "                     max_length=MAX_LENGTH)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, \n",
    "                                 batched=True, \n",
    "                                 remove_columns=[\"dialog\", \"emotion\", \"sentiment\"])\n",
    "                                 \n",
    "# Note: Since DailyDialog is large, we'll use a small subset for quick demonstration\n",
    "# You can remove the .select(...) line for full training.\n",
    "train_subset = tokenized_datasets[\"train\"].select(range(5000)) # Use first 5000 examples\n",
    "validation_subset = tokenized_datasets[\"validation\"].select(range(500)) # Use first 500 examples\n",
    "\n",
    "print(f\"üìù Training on {len(train_subset)} samples.\")\n",
    "\n",
    "# 6Ô∏è‚É£ Data Collator (Same as before, ensuring mlm=False for Causal LM)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "print(\"‚úÖ Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7Ô∏è‚É£ Define Training Arguments (Modified)\n",
    "REPO_NAME = \"gpt2-dailydialog-finetuned-demo\" # Unique name for your project\n",
    "OUTPUT_DIR = \"./results_dailydialog\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,  # Set to 1 epoch for a fast run, increase to 3-5 for better results\n",
    "    per_device_train_batch_size=4, # Reduced batch size due to larger model (GPT-2)\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # push_to_hub=True, # Uncomment if you want to push to Hub automatically\n",
    "    # hub_model_id=REPO_NAME, # Your Hugging Face model repository name\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 8Ô∏è‚É£ Initialize and Train the Trainer (Using the subset data)\n",
    "print(\"üöÄ Initializing Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=validation_subset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer # Pass the tokenizer for padding/saving purposes\n",
    ")\n",
    "\n",
    "print(\"üî• Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"‚úÖ Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9Ô∏è‚É£ Test the Fine-Tuned Model (Modified Prompt)\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline using the fine-tuned model\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1 # Use GPU if available\n",
    ")\n",
    "\n",
    "# New conversational prompt\n",
    "prompt = \"Customer: I need to book a flight to London next week.\"\n",
    "print(f\"Input Prompt: {prompt}\\n\")\n",
    "\n",
    "# Generate text\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_length=80,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True, # Use sampling for more creative text\n",
    "    temperature=0.7 # Control creativity (lower is more predictable)\n",
    ")\n",
    "\n",
    "print(\"üìù Generated Text:\")\n",
    "print(output[0]['generated_text'])\n",
    "\n",
    "# üîü Save Locally (Recommended)\n",
    "SAVE_PATH = \"./gpt2-dailydialog-local\"\n",
    "trainer.save_model(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "print(f\"‚úÖ Model saved locally to: {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e877871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0a2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead663f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503b27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc101f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7be54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639f83a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c849e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9b9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186eeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c37b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8405f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1e44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ee5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce298e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd6b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c9efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb604a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229f417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183cc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
