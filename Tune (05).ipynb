{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6529f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "317b3ee2",
   "metadata": {},
   "source": [
    "This is the Python code needed to load a cooking dataset, add your custom structural tokens, tokenize the data, and prepare it for fine-tuning GPT-2 in Colab.\n",
    "\n",
    "1. Install Libraries and Load Tokenizer ðŸ“¦\n",
    "Start by ensuring you have the necessary libraries and load the standard GPT-2 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186546e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241754e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 1. Load the Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# Set the padding token to be the same as the end-of-sequence token (standard for GPT-style models)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef6caf",
   "metadata": {},
   "source": [
    "2. Add Custom Special Tokens ðŸ”–\n",
    "To teach the model the structure of a recipe, you must add special tokens for the Title, Ingredients, Instructions, and the end of the recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e877871",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Define and Add Custom Special Tokens\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': [\n",
    "        '<|title|>', \n",
    "        '<|ingredients|>', \n",
    "        '<|instructions|>', \n",
    "        '<|endofrecipe|>'\n",
    "    ]\n",
    "}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# You MUST resize the model's token embeddings to account for the new tokens\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer)) \n",
    "print(f\"Added {num_added_toks} new tokens to the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f136186",
   "metadata": {},
   "source": [
    "3. Load, Format, and Tokenize the Dataset ðŸš\n",
    "For this example, we'll use a placeholder for a Hugging Face dataset, and define the formatting logic. You will need to replace the data loading line with your chosen cooking dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0a2e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Load a Cooking Dataset (Example: Using a small text file you create/upload)\n",
    "# If your data is in a file named 'recipes.txt', use the following:\n",
    "# raw_datasets = load_dataset('text', data_files={'train': 'recipes.txt'})\n",
    "\n",
    "# If using a Hugging Face dataset (e.g., a simple recipes collection):\n",
    "# **Replace 'corbt/all-recipes' with your chosen dataset name if needed.**\n",
    "# Ensure the dataset has a 'text' column or adapt the code below.\n",
    "raw_datasets = load_dataset('corbt/all-recipes', split='train[:1%]') # Using a small slice for quick testing\n",
    "\n",
    "# Filter out empty entries if the dataset is large and messy\n",
    "raw_datasets = raw_datasets.filter(lambda x: x['text'] is not None and len(x['text'].strip()) > 0)\n",
    "\n",
    "\n",
    "# 4. Define the Tokenization and Formatting Function\n",
    "# This function assumes each entry (row) in the dataset is one complete, formatted recipe string.\n",
    "def tokenize_and_format(examples):\n",
    "    # This is the core data preparation step: \n",
    "    # The tokenizer handles the text-to-ID conversion, truncation, and padding.\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        max_length=512, # Maximum context length for GPT-2\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "# 5. Apply the function to the dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_and_format, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# 6. Create the Train/Validation Split\n",
    "split_datasets = tokenized_datasets.train_test_split(test_size=0.1)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a044efa",
   "metadata": {},
   "source": [
    "4. Initialize Trainer and Train ðŸš€\n",
    "Now you can use the properly defined train_dataset and eval_dataset variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead663f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 7. Setup Data Collator for Causal Language Modeling (CLM)\n",
    "# mlm=False tells the collator this is for next-token prediction (GPT-2 style).\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 8. Set up Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./recipe_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4, # Adjust based on your Colab GPU memory\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./recipe_logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# 9. Initialize and Start Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# trainer.train() \n",
    "# Uncomment the line above to start the fine-tuning process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503b27e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc101f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7be54f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639f83a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c849e6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9b9f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186eeec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c37b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8405f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1e44e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ee5a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce298e88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd6b78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c9efc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb604a4d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229f417",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183cc14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
